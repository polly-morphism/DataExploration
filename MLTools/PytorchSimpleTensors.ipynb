{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31629143.230271712\n",
      "1 30145924.4809203\n",
      "2 31475047.921706647\n",
      "3 30754872.41555722\n",
      "4 25498041.97674357\n",
      "5 17355052.088619765\n",
      "6 10023466.23845114\n",
      "7 5397960.953491015\n",
      "8 3014079.1734273266\n",
      "9 1868251.825288372\n",
      "10 1300790.3471926434\n",
      "11 991963.4961957491\n",
      "12 801551.0885904575\n",
      "13 669925.1499232103\n",
      "14 571054.7984124358\n",
      "15 492635.4009926208\n",
      "16 428521.5531974027\n",
      "17 375110.3364900559\n",
      "18 330072.71289263817\n",
      "19 291744.82258327527\n",
      "20 258897.98461155486\n",
      "21 230637.90020753688\n",
      "22 206145.01135091012\n",
      "23 184816.1891783298\n",
      "24 166158.49183457997\n",
      "25 149787.97060170228\n",
      "26 135362.83627638902\n",
      "27 122617.64987001538\n",
      "28 111312.7027703601\n",
      "29 101249.39311213192\n",
      "30 92276.2264499793\n",
      "31 84251.747673239\n",
      "32 77049.48192674946\n",
      "33 70573.20578952547\n",
      "34 64738.211488148896\n",
      "35 59475.28574353219\n",
      "36 54712.98902773705\n",
      "37 50396.150504876656\n",
      "38 46480.63535438895\n",
      "39 42923.85115472696\n",
      "40 39683.38779710281\n",
      "41 36731.312941534154\n",
      "42 34036.774768394615\n",
      "43 31573.909920575694\n",
      "44 29321.1881802286\n",
      "45 27252.568136213333\n",
      "46 25353.933398834666\n",
      "47 23609.65774914994\n",
      "48 22003.19905664933\n",
      "49 20523.2210337177\n",
      "50 19157.293857515466\n",
      "51 17896.883830530765\n",
      "52 16732.277045630133\n",
      "53 15654.039287617179\n",
      "54 14655.85600878884\n",
      "55 13730.346157544034\n",
      "56 12872.028134080809\n",
      "57 12075.984651409515\n",
      "58 11335.468126130057\n",
      "59 10646.705417784386\n",
      "60 10005.914694300796\n",
      "61 9409.095864393532\n",
      "62 8853.130412855593\n",
      "63 8334.925864375684\n",
      "64 7851.121616998273\n",
      "65 7399.246084166435\n",
      "66 6977.154444280361\n",
      "67 6582.476633782727\n",
      "68 6213.419382429805\n",
      "69 5867.729211892787\n",
      "70 5544.00012947138\n",
      "71 5240.589672102888\n",
      "72 4956.0718913088\n",
      "73 4689.065575419165\n",
      "74 4438.448614579462\n",
      "75 4203.0378979541365\n",
      "76 3981.8190020901666\n",
      "77 3773.7665705755235\n",
      "78 3578.022061006648\n",
      "79 3393.7413636637084\n",
      "80 3220.213072629412\n",
      "81 3056.744872842356\n",
      "82 2902.6429266987543\n",
      "83 2757.257944816515\n",
      "84 2620.1382433688386\n",
      "85 2490.6885389535587\n",
      "86 2368.5260426406267\n",
      "87 2253.3272672690155\n",
      "88 2144.3969370015357\n",
      "89 2041.4313376882421\n",
      "90 1944.0308329589075\n",
      "91 1851.8799373750987\n",
      "92 1764.5975672245954\n",
      "93 1681.9352488321651\n",
      "94 1603.6034865462225\n",
      "95 1529.4198793539704\n",
      "96 1459.0749222593824\n",
      "97 1392.3236352503866\n",
      "98 1328.9953778212562\n",
      "99 1268.8728295965034\n",
      "100 1211.769978400762\n",
      "101 1157.5221567857807\n",
      "102 1105.9671143598716\n",
      "103 1056.9728442915516\n",
      "104 1010.3732212609093\n",
      "105 966.0397403036657\n",
      "106 923.8640226707689\n",
      "107 883.7257000557368\n",
      "108 845.4939343013865\n",
      "109 809.0824968150691\n",
      "110 774.4071746789446\n",
      "111 741.3766400662228\n",
      "112 709.8800253967163\n",
      "113 679.8512035523484\n",
      "114 651.207063243977\n",
      "115 623.8820906843282\n",
      "116 597.8077437601855\n",
      "117 572.92231297883\n",
      "118 549.1648126114139\n",
      "119 526.4747296191263\n",
      "120 504.80046710241635\n",
      "121 484.0948726413593\n",
      "122 464.308595835908\n",
      "123 445.3956524322141\n",
      "124 427.31342383358907\n",
      "125 410.0242319238188\n",
      "126 393.4861631254545\n",
      "127 377.6661511049988\n",
      "128 362.52984272429114\n",
      "129 348.0430155740471\n",
      "130 334.17696672916037\n",
      "131 320.90397621804453\n",
      "132 308.1948416740497\n",
      "133 296.0202446998137\n",
      "134 284.3604714848268\n",
      "135 273.19172458603924\n",
      "136 262.4873898410777\n",
      "137 252.23047918391794\n",
      "138 242.40073814279458\n",
      "139 232.97446950458382\n",
      "140 223.93712154282161\n",
      "141 215.2721658338602\n",
      "142 206.96243517363916\n",
      "143 198.99057624466553\n",
      "144 191.34437636066104\n",
      "145 184.0073289023465\n",
      "146 176.9662751285681\n",
      "147 170.21001893995864\n",
      "148 163.7244198391441\n",
      "149 157.49803845535587\n",
      "150 151.52086993513996\n",
      "151 145.7813928261324\n",
      "152 140.27030854263063\n",
      "153 134.97754622686017\n",
      "154 129.893730910852\n",
      "155 125.00940936960352\n",
      "156 120.31738812800945\n",
      "157 115.80986705806734\n",
      "158 111.47821409472812\n",
      "159 107.31513720276762\n",
      "160 103.31415785177074\n",
      "161 99.46838933610792\n",
      "162 95.77186535519877\n",
      "163 92.21755410514584\n",
      "164 88.80024625349735\n",
      "165 85.51457767761524\n",
      "166 82.35491508042679\n",
      "167 79.31664526924399\n",
      "168 76.3946055149703\n",
      "169 73.58331674737289\n",
      "170 70.87926064091604\n",
      "171 68.2784376783521\n",
      "172 65.77587850805091\n",
      "173 63.36798112362171\n",
      "174 61.05106540455731\n",
      "175 58.821581681674175\n",
      "176 56.67646095173832\n",
      "177 54.611566349632355\n",
      "178 52.624060003522835\n",
      "179 50.71116619631377\n",
      "180 48.869686640887195\n",
      "181 47.09704602655434\n",
      "182 45.3906241025491\n",
      "183 43.74763399874113\n",
      "184 42.166022443870084\n",
      "185 40.642971818793384\n",
      "186 39.17605376308754\n",
      "187 37.76357891161974\n",
      "188 36.40319174041732\n",
      "189 35.09309537057024\n",
      "190 33.83147803718383\n",
      "191 32.61599649155555\n",
      "192 31.445309876105355\n",
      "193 30.317603431918382\n",
      "194 29.231290560050418\n",
      "195 28.184792364431217\n",
      "196 27.176509402575462\n",
      "197 26.205007151306734\n",
      "198 25.269142766043032\n",
      "199 24.367212049617017\n",
      "200 23.498140135892367\n",
      "201 22.66076076371942\n",
      "202 21.85372508842387\n",
      "203 21.07614258479762\n",
      "204 20.326585507615086\n",
      "205 19.60420289809037\n",
      "206 18.908049310187394\n",
      "207 18.237000605581038\n",
      "208 17.590195545884953\n",
      "209 16.96671760026353\n",
      "210 16.365697285226766\n",
      "211 15.786388956623968\n",
      "212 15.227934411059735\n",
      "213 14.689505337749676\n",
      "214 14.170436148526619\n",
      "215 13.669956477900953\n",
      "216 13.187499677591678\n",
      "217 12.72228784524745\n",
      "218 12.27370848123926\n",
      "219 11.841209127528824\n",
      "220 11.424179752758551\n",
      "221 11.02201296698197\n",
      "222 10.634269211454477\n",
      "223 10.26028489264916\n",
      "224 9.899690394925303\n",
      "225 9.551880609733502\n",
      "226 9.216458332012747\n",
      "227 8.89299418088564\n",
      "228 8.581007828009103\n",
      "229 8.28018935653959\n",
      "230 7.990005953235148\n",
      "231 7.710073521802212\n",
      "232 7.440126770324422\n",
      "233 7.179682811206588\n",
      "234 6.928471126702627\n",
      "235 6.686133059806952\n",
      "236 6.452386165400379\n",
      "237 6.226910093819095\n",
      "238 6.009377533885695\n",
      "239 5.799539280344892\n",
      "240 5.59710225941121\n",
      "241 5.401806574725347\n",
      "242 5.213407477466859\n",
      "243 5.0316222850191235\n",
      "244 4.8562571553121545\n",
      "245 4.687049708042938\n",
      "246 4.523796365927304\n",
      "247 4.366287991169834\n",
      "248 4.214318926404329\n",
      "249 4.067689086036529\n",
      "250 3.926195987650548\n",
      "251 3.789668226903985\n",
      "252 3.6579384856909707\n",
      "253 3.5308357433536512\n",
      "254 3.4081735791036207\n",
      "255 3.2898138721610275\n",
      "256 3.175593830619551\n",
      "257 3.0653792974722402\n",
      "258 2.9590119889838045\n",
      "259 2.8563728790557565\n",
      "260 2.7573234191352327\n",
      "261 2.661726962890663\n",
      "262 2.5694721643748033\n",
      "263 2.480440138167835\n",
      "264 2.3945268395046204\n",
      "265 2.311598659402525\n",
      "266 2.2315882079432017\n",
      "267 2.15435966885566\n",
      "268 2.0798236177463787\n",
      "269 2.007881472348373\n",
      "270 1.9384399248055506\n",
      "271 1.8714204294787617\n",
      "272 1.8067353365387882\n",
      "273 1.744300682913226\n",
      "274 1.6840370993443405\n",
      "275 1.625871090657053\n",
      "276 1.5697230013801602\n",
      "277 1.5155286316206436\n",
      "278 1.4632096770423673\n",
      "279 1.412720741432756\n",
      "280 1.3639731305013378\n",
      "281 1.3169219580472262\n",
      "282 1.2715025779903542\n",
      "283 1.227660062844307\n",
      "284 1.1853364410261227\n",
      "285 1.1444804727636015\n",
      "286 1.1050397233383338\n",
      "287 1.0669713374702323\n",
      "288 1.0302116587674472\n",
      "289 0.9947310469446498\n",
      "290 0.9604779727297346\n",
      "291 0.9274117107851398\n",
      "292 0.8954865283352123\n",
      "293 0.8646663810881674\n",
      "294 0.834914675808478\n",
      "295 0.8061901990543903\n",
      "296 0.7784586777983756\n",
      "297 0.7516863703058588\n",
      "298 0.7258392070567525\n",
      "299 0.7008869774273169\n",
      "300 0.6768021396408906\n",
      "301 0.6535412674901762\n",
      "302 0.631083656385093\n",
      "303 0.6093988035498512\n",
      "304 0.5884649971832054\n",
      "305 0.5682533521122857\n",
      "306 0.5487376606330859\n",
      "307 0.529895513968457\n",
      "308 0.5117026383558843\n",
      "309 0.4941389470667018\n",
      "310 0.4771789531280245\n",
      "311 0.4608038555504407\n",
      "312 0.44499207184831874\n",
      "313 0.4297257622116646\n",
      "314 0.4149852288882989\n",
      "315 0.40075209816600854\n",
      "316 0.3870091462285449\n",
      "317 0.37373948421819475\n",
      "318 0.36092506188510937\n",
      "319 0.3485535927885618\n",
      "320 0.3366068851286755\n",
      "321 0.3250712214097571\n",
      "322 0.3139317176079819\n",
      "323 0.30317690177450407\n",
      "324 0.29279037744097414\n",
      "325 0.28276064370103593\n",
      "326 0.27307725971020336\n",
      "327 0.26372548803854845\n",
      "328 0.2546951189542834\n",
      "329 0.2459749082593768\n",
      "330 0.2375545154010722\n",
      "331 0.22942315450923276\n",
      "332 0.22157295059803236\n",
      "333 0.2139915311827557\n",
      "334 0.20666919316605997\n",
      "335 0.19959814709019297\n",
      "336 0.19277015066273884\n",
      "337 0.18617620486141584\n",
      "338 0.17980842391579346\n",
      "339 0.17365898020726597\n",
      "340 0.16772040481274364\n",
      "341 0.1619858771136678\n",
      "342 0.15644764680697137\n",
      "343 0.1510991849755193\n",
      "344 0.1459344443361048\n",
      "345 0.14094640443563233\n",
      "346 0.136129888292954\n",
      "347 0.13147770629910382\n",
      "348 0.1269855845506706\n",
      "349 0.12264717994031345\n",
      "350 0.11845705522990667\n",
      "351 0.1144109288318705\n",
      "352 0.11050282195868359\n",
      "353 0.10672900226590301\n",
      "354 0.10308403022957338\n",
      "355 0.09956436361238222\n",
      "356 0.09616482017218922\n",
      "357 0.09288144384788768\n",
      "358 0.08971076019694568\n",
      "359 0.08664847571014905\n",
      "360 0.08369103618257878\n",
      "361 0.08083486619784241\n",
      "362 0.0780763253646086\n",
      "363 0.07541218737828174\n",
      "364 0.07283996180442177\n",
      "365 0.07035489101207874\n",
      "366 0.06795485411376734\n",
      "367 0.06563655089079762\n",
      "368 0.06339779389442646\n",
      "369 0.06123551238564878\n",
      "370 0.05914702060885115\n",
      "371 0.057129995094085326\n",
      "372 0.055181943862476275\n",
      "373 0.053300493952619724\n",
      "374 0.051483179346176036\n",
      "375 0.04972813769344593\n",
      "376 0.04803300446031396\n",
      "377 0.046395655213948894\n",
      "378 0.04481430910557725\n",
      "379 0.043286962127258584\n",
      "380 0.0418117491726107\n",
      "381 0.04038693376720322\n",
      "382 0.03901072804263614\n",
      "383 0.03768167241808153\n",
      "384 0.036397806734286914\n",
      "385 0.03515778689658538\n",
      "386 0.0339602022414232\n",
      "387 0.03280331997575496\n",
      "388 0.031686101095763676\n",
      "389 0.030606877154788534\n",
      "390 0.029564582464678083\n",
      "391 0.02855775989320799\n",
      "392 0.027585262233761537\n",
      "393 0.026646051680906422\n",
      "394 0.025738765147108815\n",
      "395 0.02486286682717581\n",
      "396 0.024016465686521986\n",
      "397 0.023198906826796525\n",
      "398 0.022409239883193023\n",
      "399 0.021646453353311286\n",
      "400 0.020909761023957536\n",
      "401 0.020198111261926357\n",
      "402 0.01951075273927687\n",
      "403 0.018846817975659146\n",
      "404 0.01820551483618254\n",
      "405 0.017586091883737158\n",
      "406 0.016987751397995004\n",
      "407 0.01640976709850907\n",
      "408 0.015851557366893428\n",
      "409 0.015312317490648868\n",
      "410 0.014791436465704008\n",
      "411 0.014288321193267202\n",
      "412 0.013802359188707591\n",
      "413 0.013332949354443862\n",
      "414 0.012879479028049447\n",
      "415 0.012441518579005698\n",
      "416 0.01201843751381087\n",
      "417 0.011609772584669397\n",
      "418 0.011215033482033634\n",
      "419 0.010833733152655792\n",
      "420 0.010465418442150515\n",
      "421 0.01010960218273141\n",
      "422 0.009765953223864416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 0.00943399576029019\n",
      "424 0.009113301067892852\n",
      "425 0.008803561649895434\n",
      "426 0.008504467363864818\n",
      "427 0.008215439508911678\n",
      "428 0.007936233772373801\n",
      "429 0.007666538018487659\n",
      "430 0.0074060252206091335\n",
      "431 0.007154365343948463\n",
      "432 0.0069112829634253945\n",
      "433 0.006676461677608147\n",
      "434 0.006449639507762502\n",
      "435 0.006230515209877285\n",
      "436 0.006018858771908964\n",
      "437 0.00581438904037085\n",
      "438 0.00561688456494919\n",
      "439 0.005426092448126465\n",
      "440 0.00524178359122459\n",
      "441 0.00506376432160815\n",
      "442 0.0048917769128950135\n",
      "443 0.004725647042159774\n",
      "444 0.004565153488681954\n",
      "445 0.004410143944723641\n",
      "446 0.004260387799391044\n",
      "447 0.004115715045875267\n",
      "448 0.003975976444924341\n",
      "449 0.0038409833441644973\n",
      "450 0.003710578813309512\n",
      "451 0.0035845995042134977\n",
      "452 0.003462910596908739\n",
      "453 0.0033453546346214873\n",
      "454 0.003231787894275557\n",
      "455 0.0031220860783824937\n",
      "456 0.0030161186274221107\n",
      "457 0.0029137829550458672\n",
      "458 0.0028148927164275066\n",
      "459 0.0027193574862133963\n",
      "460 0.0026270703520721033\n",
      "461 0.0025379145636930524\n",
      "462 0.002451792217956828\n",
      "463 0.0023685938758859866\n",
      "464 0.002288224192669798\n",
      "465 0.0022105776759118625\n",
      "466 0.002135571157063091\n",
      "467 0.002063112454182909\n",
      "468 0.0019931193986419296\n",
      "469 0.0019254983163221537\n",
      "470 0.0018601701342525708\n",
      "471 0.0017970721926416366\n",
      "472 0.0017361081599325343\n",
      "473 0.0016772170785596292\n",
      "474 0.0016203223404804702\n",
      "475 0.0015653644828128412\n",
      "476 0.0015122713571217223\n",
      "477 0.0014609746452760542\n",
      "478 0.0014114291726693197\n",
      "479 0.0013635630398927251\n",
      "480 0.0013173184469434988\n",
      "481 0.001272645411913121\n",
      "482 0.0012294861647980755\n",
      "483 0.0011877958188643285\n",
      "484 0.0011475178472082873\n",
      "485 0.0011086089465947285\n",
      "486 0.0010710197433423494\n",
      "487 0.00103470582991161\n",
      "488 0.0009996394164334503\n",
      "489 0.0009657490804594286\n",
      "490 0.0009330086151554272\n",
      "491 0.000901379455852586\n",
      "492 0.0008708214213229829\n",
      "493 0.0008412997873975206\n",
      "494 0.0008127844903947079\n",
      "495 0.0007852326045428809\n",
      "496 0.0007586160515256567\n",
      "497 0.0007329022717271508\n",
      "498 0.0007080623837025925\n",
      "499 0.0006840646313286502\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 285.3568115234375\n",
      "199 0.6920117735862732\n",
      "299 0.0030455845408141613\n",
      "399 0.00011420878581702709\n",
      "499 2.8979862690903246e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 477.5949401855469\n",
      "199 2.884270191192627\n",
      "299 0.02897120825946331\n",
      "399 0.0005837358185090125\n",
      "499 7.093205204000697e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 557.616455078125\n",
      "199 3.0563199520111084\n",
      "299 0.024973753839731216\n",
      "399 0.00047791062388569117\n",
      "499 6.597527681151405e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.440094470977783\n",
      "199 0.045455366373062134\n",
      "299 0.0016400543972849846\n",
      "399 7.65000149840489e-05\n",
      "499 4.0681638893147465e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 77.94659423828125\n",
      "199 2.4293158054351807\n",
      "299 0.04390353336930275\n",
      "399 0.0002492743660695851\n",
      "499 3.801491459398676e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.2893168926239014\n",
      "199 0.041456300765275955\n",
      "299 0.0019597846549004316\n",
      "399 0.00013569873408414423\n",
      "499 1.0640556865837425e-05\n"
     ]
    }
   ],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 34.45521545410156\n",
      "199 7.00022029876709\n",
      "299 3.453885793685913\n",
      "399 0.33409249782562256\n",
      "499 1.5149109363555908\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
